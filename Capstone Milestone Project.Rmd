---
title: "Coursera Data Science Capstone Milestone Report March 2016"
author: "cfkhor@gmail.com"
output:
  html_document:
    keep_md: yes
---

##Introduction

This milestone report is based on exploratory data analysis of the SwifKey data provided by Coursera Data Science Capstone. The language selected is English and consist of 3 text files containing text from three different sources (blogs, news & twitter). Some of the code is hidden to preserve space, but can be accessed by looking at the Raw .Rmd, which can be found in my GitHub repository https://github.com/cfkhor/Milestone_Project

##Assumptions

It is assumed that the data has been downloaded, unzipped and placed into the active R directory, maintaining the folder structure. 

##Reference
Efficient R code Ngram tokenizer written by Maciej Szymkiewicz for our analysis. https://github.com/zero323/r-snippets/blob/master/R/ngram_tokenizer.R
basic understanding of NLP https://en.wikipedia.org/wiki/N-gram

##Raw Data Summary

Summary of the three input files. The numbers have been calculated by using the `wc` command.

```{r,echo=FALSE, cache=TRUE}
setwd("~/Documents/Coursera/Capstone/final/en_US")

lines <- system("wc -l *.txt", intern=TRUE)
words <- system("wc -w *.txt", intern=TRUE)
longest <- system("wc -L *.txt", intern=TRUE)

lines.blogs <- as.numeric(gsub('[^0-9]', '', lines[1]))
lines.news <- as.numeric(gsub('[^0-9]', '', lines[2]))
lines.twitter <- as.numeric(gsub('[^0-9]', '', lines[3]))

words.blogs <- as.numeric(gsub('[^0-9]', '', words[1]))
words.news <- as.numeric(gsub('[^0-9]', '', words[2]))
words.twitter <- as.numeric(gsub('[^0-9]', '', words[3]))

longest.blogs  <- as.numeric(gsub('[^0-9]', '', longest[1]))
longest.news  <- as.numeric(gsub('[^0-9]', '', longest[2]))
longest.twitter  <- as.numeric(gsub('[^0-9]', '', longest[3]))

blogs <- c(lines.blogs,words.blogs,longest.blogs)
news <- c(lines.news,words.news,longest.news)
twitter <- c(lines.twitter,words.twitter,longest.twitter)

info <- data.frame(rbind(blogs, news, twitter))

names(info) <- c("Word Count", "Line Count", "Longest Line")

setwd("~/Documents/Coursera/Capstone/final/en_US")

info
```

##Load Data & Sample

Next, we load the data into R so we can manipulate them. We use readLines to load blogs and twitter, but we load news in binomial mode as it contains special characters.

```{r, cache=TRUE, results='hide', warning=FALSE}
# Set the correct working directory
setwd("~/Documents/Coursera/Capstone/final/en_US")

# Reading the blogs and twitter files
source.blogs <- readLines("en_US.blogs.txt", encoding="UTF-8")
source.twitter <- readLines("en_US.twitter.txt", encoding="UTF-8")

# Reading the news file. using binary mode as there are special characters
con <- file("en_US.news.txt", open="rb")
source.news <- readLines(con, encoding="UTF-8")
close(con)
rm(con)
```

Next, we take sample of each file, because running the calculations using the raw files is not feasible and will be slow.  Using a binomial function to take sample. Binomial sampling is equal to flipping  a coin to decide which lines we should include. We decide to include 2% of each text file.

```{r, cache=TRUE}
setwd("~/Documents/Coursera/Capstone/final/en_US")

# Binomial sampling of the data and create the relevant files
sample.fun <- function(data, percent)
{
  return(data[as.logical(rbinom(length(data),2,percent))])
}

# Remove all non english characters as they cause issues down the road
source.blogs <- iconv(source.blogs, "latin1", "ASCII", sub="")
source.news <- iconv(source.news, "latin1", "ASCII", sub="")
source.twitter <- iconv(source.twitter, "latin1", "ASCII", sub="")

# Set the desired sample percentage
percentage <- 0.2

sample.blogs   <- sample.fun(source.blogs, percentage)
sample.news   <- sample.fun(source.news, percentage)
sample.twitter   <- sample.fun(source.twitter, percentage)

dir.create("sample", showWarnings = FALSE)

write(sample.blogs, "sample/sample.blogs.txt")
write(sample.news, "sample/sample.news.txt")
write(sample.twitter, "sample/sample.twitter.txt")

remove(source.blogs)
remove(source.news)
remove(source.twitter)
```

##Sample Summary

A summary for the sample can be seen on the table below.

```{r,echo=FALSE, cache=TRUE}
setwd("~/Documents/Coursera/Capstone/final/en_US")

lines <- system("wc -l *.txt", intern=TRUE)
words <- system("wc -w *.txt", intern=TRUE)
longest <- system("wc -L *.txt", intern=TRUE)

lines.blogs <- as.numeric(gsub('[^0-9]', '', lines[1]))
lines.news <- as.numeric(gsub('[^0-9]', '', lines[2]))
lines.twitter <- as.numeric(gsub('[^0-9]', '', lines[3]))

words.blogs <- as.numeric(gsub('[^0-9]', '', words[1]))
words.news <- as.numeric(gsub('[^0-9]', '', words[2]))
words.twitter <- as.numeric(gsub('[^0-9]', '', words[3]))

longest.blogs  <- as.numeric(gsub('[^0-9]', '', longest[1]))
longest.news  <- as.numeric(gsub('[^0-9]', '', longest[2]))
longest.twitter  <- as.numeric(gsub('[^0-9]', '', longest[3]))

blogs <- c(lines.blogs,words.blogs,longest.blogs)
news <- c(lines.news,words.news,longest.news)
twitter <- c(lines.twitter,words.twitter,longest.twitter)

info <- data.frame(rbind(blogs, news, twitter))

names(info) <- c("Word Count", "Line Count", "Longest Line")

setwd("~/Documents/Coursera/Capstone/final/en_US")

info
```

## Create & Clean a Corpus

In order to be able to clean and manipulate the data, we will create a corpus, which will combined all three sample text files

```{r, cache=TRUE, results='hide'}
library(tm)
library(RWeka)
library(SnowballC)
sample.corpus <- c(sample.blogs,sample.news,sample.twitter)
my.corpus <- Corpus(VectorSource(list(sample.corpus)))
```

Next.cleaning it. In order to do that, we will transform all characters to lowercase, we will remove the punctuation, remove the numbers and the common english stopwords (and, the, or etc..)

```{r, cache=TRUE}
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
```

We also need to remove profanity. To do that we will use the google bad words database.

```{r, cache=TRUE}
setwd("~/Documents/Coursera/Capstone/final/en_US")
googlebadwords <- read.delim("google_bad_words.txt",sep = ":",header = FALSE)
googlebadwords <- googlebadwords[,1]
my.corpus <- tm_map(my.corpus, removeWords, googlebadwords)
```

Finally, we will strip the excess white space

```{r, cache=TRUE}
my.corpus <- tm_map(my.corpus, stripWhitespace)
```

Before moving to the next step, we will save the corpus in a text file so we have it intact for future reference.

```{r, cache=TRUE}
setwd("~/Documents/Coursera/Capstone/final/en_US")
writeCorpus(my.corpus, filenames="my.corpus.txt")
my.corpus <- readLines("my.corpus.txt")
```

## Unigram Analysis

The first analysis we will perform is a unigram analysis. This will show us which words are frequently used. We use the Ngrams_tokenizer that Maciej Szymkiewicz kindly made public. We will  pass the argumemnt 1 to get the unigrams. This will create a unigram Dataframe, which we will then manipulate so we can chart the frequencies using ggplot.

```{r, cache=TRUE}
setwd("~/Documents/Coursera/Capstone/Milestone Project")
library(ggplot2)
source("Ngrams_tokenizer.R")
unigram.tokenizer <- ngram_tokenizer(1)
wordlist <- unigram.tokenizer(my.corpus)
unigram.df <- data.frame(V1 = as.vector(names(table(unlist(wordlist)))), V2 = as.numeric(table(unlist(wordlist))))
names(unigram.df) <- c("word","freq")
unigram.df <- unigram.df[with(unigram.df, order(-unigram.df$freq)),]
row.names(unigram.df) <- NULL
save(unigram.df, file="unigram.Rda")
```

```{r, fig.width=20, fig.height=7, cache=TRUE}
ggplot(head(unigram.df,15), aes(x=reorder(word,-freq), y=freq)) +
  geom_bar(stat="Identity", fill="blue") +
  geom_text(aes(label=freq), vjust = -0.5) +
  ggtitle("Unigrams frequency") +
  ylab("Frequency") +
  xlab("Term")
```

## Bigram Analysis

Next, we will do Bigrams, i.e. two word combinations. Same process is followed, but this time we will pass in the argument 2.

```{r, cache=TRUE}
bigram.tokenizer <- ngram_tokenizer(2)
wordlist <- bigram.tokenizer(my.corpus)
bigram.df <- data.frame(V1 = as.vector(names(table(unlist(wordlist)))), V2 = as.numeric(table(unlist(wordlist))))
names(bigram.df) <- c("word","freq")
bigram.df <- bigram.df[with(bigram.df, order(-bigram.df$freq)),]
row.names(bigram.df) <- NULL
setwd("~/Documents/Coursera/Capstone/final/en_US")
save(bigram.df, file="bigram.Rda")
```

```{r, fig.width=20, fig.height=7, cache=TRUE}
ggplot(head(bigram.df,15), aes(x=reorder(word,-freq), y=freq)) +
  geom_bar(stat="Identity", fill="blue") +
  geom_text(aes(label=freq), vjust = -0.5) +
  ggtitle("Bigrams frequency") +
  ylab("Frequency") +
  xlab("Term")
```

## Trigram Analysis

Last but not least three word combinations.

```{r, cache=TRUE}
trigram.tokenizer <- ngram_tokenizer(3)
wordlist <- trigram.tokenizer(my.corpus)
trigram.df <- data.frame(V1 = as.vector(names(table(unlist(wordlist)))), V2 = as.numeric(table(unlist(wordlist))))
names(trigram.df) <- c("word","freq")
trigram.df <- trigram.df[with(trigram.df, order(-trigram.df$freq)),]
row.names(trigram.df) <- NULL
save(trigram.df, file="trigram.Rda")
```

```{r, fig.width=20, fig.height=7, cache=TRUE}
ggplot(head(trigram.df,15), aes(x=reorder(word,-freq), y=freq)) +
  geom_bar(stat="Identity", fill="blue") +
  geom_text(aes(label=freq), vjust = -0.5) +
  ggtitle("Trigrams frequency") +
  ylab("Frequency") +
  xlab("Term")
```

```{r, echo=FALSE, results='hide'}
gc()
```

## Steps going forward

This is the end of data exploratory analysis. As stated in the assignment, the next step is to create a model and integrated into a Shiny app for word prediction.

We will use the Ngram dataframes created here to calculate the probability of the next word occuring. The input string will be tokenized and the last 2 (or 1 if it's a unigram) words will be isolated and cross checked against the data frames to get the highest probability of next word.

